{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "from csc.charuco_stereo_calibrator import CharucoStereoCalibrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[INFO] 2024-11-30 19:15:07 - Calibration started for stereo images.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "images_left = glob.glob(\"samples/checker/*.jpg\")\n",
    "images_right = glob.glob(\"samples/checker_r/*.jpg\")\n",
    "\n",
    "chessboard_size = (11, 8)\n",
    "frame_size_h = 2592 // 2\n",
    "frame_size_w = 4608 // 2\n",
    "\n",
    "# if below is None, then algorithm figure this out.\n",
    "f_in_mm = 4.74\n",
    "pixel_size_mm = 1.4e-3 * 2  # binning\n",
    "\n",
    "stereo_calibrator = CharucoStereoCalibrator(\n",
    "    chessboard_size=chessboard_size,\n",
    "    frame_size_h=frame_size_h,\n",
    "    frame_size_w=frame_size_w,\n",
    "    f_in_mm=f_in_mm,\n",
    "    pixel_size_mm=pixel_size_mm,\n",
    "    debug=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[INFO] 2024-11-30 19:15:07 - Starting stereo image processing...\u001b[0m\n",
      "\n",
      "==================================================\n",
      "âœ…KNOWN CAMERA INTRINSICS:\n",
      "==================================================\n",
      "\n",
      "[[1692.8571 0.0000 1152.0000]\n",
      " [0.0000 1692.8571 648.0000]\n",
      " [0.0000 0.0000 1.0000]]\n",
      "\n",
      "==================================================\n",
      "\u001b[94m[INFO] 2024-11-30 19:15:10 - Calibrating the left camera...\u001b[0m\n",
      "\u001b[92m[SUCCESS] 2024-11-30 19:15:10 - ðŸŽ¥ Left Camera Calibration RMS Error: 1.2554\u001b[0m\n",
      "\n",
      "==================================================\n",
      "LEFT CAMERA MATRIX:\n",
      "==================================================\n",
      "\n",
      "[[1692.8571 0.0000 1152.0000]\n",
      " [0.0000 1692.8571 648.0000]\n",
      " [0.0000 0.0000 1.0000]]\n",
      "\n",
      "==================================================\n",
      "\u001b[94m[INFO] 2024-11-30 19:15:10 - Calibrating the right camera...\u001b[0m\n",
      "\u001b[92m[SUCCESS] 2024-11-30 19:15:10 - ðŸŽ¥ Right Camera Calibration RMS Error: 1.2759\u001b[0m\n",
      "\n",
      "==================================================\n",
      "RIGHT CAMERA MATRIX:\n",
      "==================================================\n",
      "\n",
      "[[1692.8571 0.0000 1152.0000]\n",
      " [0.0000 1692.8571 648.0000]\n",
      " [0.0000 0.0000 1.0000]]\n",
      "\n",
      "==================================================\n",
      "\u001b[94m[INFO] 2024-11-30 19:15:10 - Performing stereo calibration...\u001b[0m\n",
      "\u001b[92m[SUCCESS] 2024-11-30 19:15:11 - ðŸŽ¥ Stereo Camera Calibration RMS Error: 1.4890\u001b[0m\n",
      "\u001b[94m[INFO] 2024-11-30 19:15:11 - Saving calibration matrices and parameters...\u001b[0m\n",
      "Saving parameters!\n",
      "All parameters saved successfully!\n",
      "\u001b[92m[SUCCESS] 2024-11-30 19:15:12 - Stereo calibration completed successfully!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "stereo_calibrator.perform_calibration(images_left, images_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,0:  [[20. 20.  0.]]\n",
      "0,1:  [[40. 20.  0.]]\n",
      "0,2:  [[60. 20.  0.]]\n",
      "1,0:  [[20. 20.  0.]]\n",
      "====================\n",
      "0,0:  [[20. 20.  0.]]\n",
      "0,1:  [[40. 20.  0.]]\n",
      "0,2:  [[60. 20.  0.]]\n",
      "1,0:  [[20. 20.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"0,0: \", stereo_calibrator.objpointsL[0][0])\n",
    "print(\"0,1: \", stereo_calibrator.objpointsL[0][1])\n",
    "print(\"0,2: \", stereo_calibrator.objpointsL[0][2])\n",
    "print(\"1,0: \", stereo_calibrator.objpointsL[1][0])\n",
    "print(\"==\" * 10)\n",
    "print(\"0,0: \", stereo_calibrator.objpointsR[0][0])\n",
    "print(\"0,1: \", stereo_calibrator.objpointsR[0][1])\n",
    "print(\"0,2: \", stereo_calibrator.objpointsR[0][2])\n",
    "print(\"1,0: \", stereo_calibrator.objpointsR[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "distance based on `square_mm` of the checker board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret, camera_matrix, dist, rvecs, tvecs = stereo_calibrator.calibrate_camera(\n",
    "    stereo_calibrator.objpointsL, stereo_calibrator.imgpointsL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **ret**:\n",
    "  - **Description**: The `ret` value, also known as the reprojection error, measures how well the calibration algorithm approximates the actual camera parameters. It is the average distance between observed image points and projected object points using estimated parameters.\n",
    "  - **Mathematical Representation**:\n",
    "\n",
    "    $$  \n",
    "    \\text{ret} = \\frac{1}{N} \\sum_{i=1}^{N} \\| \\mathbf{p}_i - \\mathbf{\\hat{p}}_i \\|\n",
    "    $$\n",
    "\n",
    "    where:\n",
    "    - $N$ is the total number of points,\n",
    "    - $ \\mathbf{p}_i $ is the observed image point,\n",
    "    - $ \\mathbf{\\hat{p}}_i $ is the projected object point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2553738901379743"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **camera_matrix**:\n",
    "  - **Description**: The `camera_matrix` $ K $ represents the camera's intrinsic parameters, including focal lengths and the optical center. It is a 3x3 matrix.\n",
    "  - **Mathematical Representation**:\n",
    "\n",
    "    $$ \n",
    "    K = \\begin{bmatrix}\n",
    "    f_x & 0 & c_x \\\\\n",
    "    0 & f_y & c_y \\\\\n",
    "    0 & 0 & 1\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "    where:\n",
    "    - $ f_x, f_y $ are the focal lengths in pixels,\n",
    "    - $ c_x, c_y $ are the optical center coordinates in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.69285714e+03, 0.00000000e+00, 1.15200000e+03],\n",
       "       [0.00000000e+00, 1.69285714e+03, 6.48000000e+02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camera_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **dist**:\n",
    "  - **Description**: The `dist` array contains distortion coefficients for lens distortion, including radial and tangential distortions.\n",
    "  - **Mathematical Representation**:\n",
    "\n",
    "    $$ \n",
    "    \\text{dist} = \\begin{bmatrix}\n",
    "    k_1 & k_2 & p_1 & p_2 & k_3\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "    where:\n",
    "    - $ k_1, k_2, k_3 $ are radial distortion coefficients,\n",
    "    - $ p_1, p_2 $ are tangential distortion coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.021592  ,  0.04840796,  0.00154551, -0.00355661, -0.18525454]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **rvecs**:\n",
    "  - **Description**: `rvecs` (rotation vectors) represent the camera's rotation relative to each view of the calibration object. Each vector can be converted into a rotation matrix using Rodrigues' formula.\n",
    "  - **Mathematical Representation**: Each $ \\mathbf{rvec}_i $ is a 3-element vector for rotation in 3D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 vectors from 22 calibration images\n",
      "first rvec: \n",
      " [[-0.61295532]\n",
      " [ 0.07442224]\n",
      " [-0.00826071]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(rvecs)} vectors from 22 calibration images\")\n",
    "print(\"first rvec: \\n\", rvecs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **tvecs**:\n",
    "  - **Description**: `tvecs` (translation vectors) represent the camera's translation relative to each view of the calibration object. They translate the origin of the world coordinate system to the camera coordinate system.\n",
    "  - **Mathematical Representation**: Each $ \\mathbf{tvec}_i $ is a 3-element vector for translation in 3D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first tvec: \n",
      " [[-149.96290099]\n",
      " [ -86.8913117 ]\n",
      " [ 431.1844847 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"first tvec: \\n\", tvecs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Given**:\n",
    "   - Intrinsic camera matrices $ K_1 $ and $ K_2 $ for two cameras.\n",
    "   - Rotation matrix $ R $ and translation vector $ \\mathbf{t} $ between the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_R, camera_matrix_R, dist_R, rvecs_R, tvecs_R = stereo_calibrator.calibrate_camera(\n",
    "    stereo_calibrator.objpointsR, stereo_calibrator.imgpointsR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fundamental Matrix (F)\n",
    "The Fundamental Matrix $F$ is a $3 \\times 3$ matrix that encapsulates the intrinsic and extrinsic parameters of the stereo camera system. It relates corresponding points between two images:\n",
    "$$ \\mathbf{x'}^T \\cdot \\mathbf{F} \\cdot \\mathbf{x} = 0 $$\n",
    "where $\\mathbf{x}$ and $\\mathbf{x'}$ are homogeneous coordinates of corresponding points in left and right images, respectively.\n",
    "\n",
    "Let's now compute the Fundamental Matrix $F$ and the Essential Matrix $E$ using the calibrated parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fundamental Matrix (F):\n",
      " [[ 5.81599683e-06 -1.14809833e-03  1.32732000e+00]\n",
      " [ 1.10956131e-03 -1.36141344e-04 -3.37529089e+00]\n",
      " [-1.23598760e+00  3.35822789e+00  1.00000000e+00]]\n",
      "mask shape:  (70, 1)\n"
     ]
    }
   ],
   "source": [
    "# Typically object points are the same for both cameras in stereo calibration\n",
    "imgpointsL = [np.array(pts, dtype=np.float32) for pts in stereo_calibrator.imgpointsL]\n",
    "imgpointsR = [np.array(pts, dtype=np.float32) for pts in stereo_calibrator.imgpointsR]\n",
    "\n",
    "# Example using a pair of points for each image (for demo purpose, ensure all points are used appropriately)\n",
    "assert len(imgpointsL) == len(\n",
    "    imgpointsR\n",
    "), \"The number of point sets must be equal for the left and right images.\"\n",
    "\n",
    "# Now calculate the Fundamental Matrix using one set of corresponding points\n",
    "F, mask = cv2.findFundamentalMat(imgpointsL[0], imgpointsR[0], cv2.FM_LMEDS)\n",
    "\n",
    "# Print the matrices\n",
    "print(\"Fundamental Matrix (F):\\n\", F)\n",
    "print(\"mask shape: \", mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general form of the Fundamental Matrix is:\n",
    "\n",
    "$$ F = \\begin{bmatrix}\n",
    "f_{11} & f_{12} & f_{13} \\\\\n",
    "f_{21} & f_{22} & f_{23} \\\\\n",
    "f_{31} & f_{32} & f_{33}\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "Each element of this matrix corresponds to a component of the transformation between corresponding points in the two images.\n",
    "\n",
    "1. **General Interpretation**: \n",
    "   - $F$  is generated using matched image points and is fundamental in describing the epipolar lines between two images.\n",
    "   - If a point $ \\mathbf{x} $ in the first image corresponds to a point $ \\mathbf{x'} $ in the second image, then $ \\mathbf{x'}^T F \\mathbf{x} = 0 $ must hold.\n",
    "\n",
    "2. **Matrix Elements**:\n",
    "   - The elements $ f_{ij} $ of the matrix do not have direct physical meaning like camera parameters. Instead, they collectively encode the epipolar geometry between two views.\n",
    "   - The matrix is rank 2, meaning its determinant is zero, which is a fundamental property used in its derivation.\n",
    "\n",
    "3. **Role in Epipolar Geometry**:\n",
    "   - The rows and columns of $ F $ can be used to compute epipolar lines.\n",
    "   - For a point $\\mathbf{x}$ in the left image, the line $ \\mathbf{l'} = F \\mathbf{x} $ is the corresponding epipolar line in the right image.\n",
    "   - Similarly, for a point $\\mathbf{x'}$ in the right image, the line $ \\mathbf{l} = F^T \\mathbf{x'} $ is the corresponding epipolar line in the left image.\n",
    "\n",
    "- **Mask Shape**: The mask's shape is $ (70, 1) $, indicating that there are 70 pairs of matched points. Each entry in the mask specifies whether the corresponding point pair was considered an inlier (1) or outlier (0) during the robust estimation process (e.g., using RANSAC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essential Matrix (E)\n",
    "The Essential Matrix $E$ relates corresponding points in a normalized (undistorted) image plane and reflects the extrinsic geometry between the cameras:\n",
    "$$ \\mathbf{x'}^T \\cdot \\mathbf{E} \\cdot \\mathbf{x} = 0 $$\n",
    "The Essential Matrix is derived from the Fundamental Matrix when camera parameters are known, using:\n",
    "$$ \\mathbf{E} = \\mathbf{K'}^T \\cdot \\mathbf{F} \\cdot \\mathbf{K} $$\n",
    "where $\\mathbf{K}$ and $\\mathbf{K'}$ are the intrinsic parameters (camera matrices) of the left and right cameras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Essential Matrix (E):\n",
      " [[   16.66728194 -3290.18034966   998.87427707]\n",
      " [ 3179.74230338  -390.14914154 -3699.39370094]\n",
      " [ -863.85119723  3296.66836033    16.94250883]]\n"
     ]
    }
   ],
   "source": [
    "# Ensure camera matrices are correctly defined\n",
    "camera_matrix_L = np.array(camera_matrix)\n",
    "camera_matrix_R = np.array(camera_matrix_R)\n",
    "\n",
    "# Compute the Essential Matrix\n",
    "E = camera_matrix_R.T @ F @ camera_matrix_L\n",
    "print(\"Essential Matrix (E):\\n\", E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ E = \\begin{bmatrix}\n",
    "e_{11} & e_{12} & e_{13} \\\\\n",
    "e_{21} & e_{22} & e_{23} \\\\\n",
    "e_{31} & e_{32} & e_{33}\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "Unlike the Fundamental Matrix, the Essential Matrix has direct geometric significance because it is computed in the normalized camera coordinate space.\n",
    "\n",
    "- $ E $ is also a rank 2 matrix, indicating that it encapsulates a planar constraint in three-dimensional space.\n",
    "- The matrix should satisfy the condition that the two non-zero singular values are equal when decomposed using SVD (Singular Value Decomposition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following calibrated parameters were obtained\n",
    "R = stereo_calibrator.rot  # Rotation matrix from stereo calibration\n",
    "T = stereo_calibrator.trans  # Translation vector from stereo calibration\n",
    "camera_matrix_L = camera_matrix  # Camera matrix for the left camera\n",
    "dist_coeffs_L = dist  # Distortion coefficients for the left camera\n",
    "camera_matrix_R = camera_matrix_R  # Camera matrix for the right camera\n",
    "dist_coeffs_R = dist_R  # Distortion coefficients for the right camera\n",
    "image_size = (frame_size_w, frame_size_h)  # Size of the images\n",
    "\n",
    "# Stereo rectification\n",
    "R1, R2, P1, P2, Q, roi1, roi2 = cv2.stereoRectify(\n",
    "    camera_matrix_L,\n",
    "    dist_coeffs_L,\n",
    "    camera_matrix_R,\n",
    "    dist_coeffs_R,\n",
    "    image_size,\n",
    "    R,\n",
    "    T,\n",
    "    alpha=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Rectification Transforms $R1, R2$**:\n",
    "   - $R1$: The rectification transformation for the first (left) camera.\n",
    "   - $R2$: The rectification transformation for the second (right) camera.\n",
    "   - **Purpose**: These matrices transform the original camera frames such that the epipolar lines become parallel and aligned along the horizontal axis. They adjust the orientation of each camera's image plane for rectification.\n",
    "\n",
    "2. **Projection Matrices $P1, P2$**:\n",
    "   - $P1$: The new projection matrix for the first camera after rectification.\n",
    "   - $P2$: The new projection matrix for the second camera after rectification.\n",
    "   - **Purpose**: These matrices define the perspective projection of 3D points into the rectified image frames. They account for the transformation applied through the rectification process and include adjustments for focal lengths and principal points, effectively detailing how the 3D world projects into each rectified image.\n",
    "\n",
    "3. **Disparity-to-Depth Mapping Matrix $Q$**:\n",
    "   - $Q$: A $4 \\times 4$ reprojection matrix.\n",
    "   - **Purpose**: This matrix is used to convert disparity values (differences in x-coordinates of corresponding points in the two images) back to 3D coordinates in real-world space. It essentially maps the 2D disparity from the rectified stereo images into 3D space, facilitating depth estimation and point cloud generation.\n",
    "\n",
    "4. **Regions of Interest $roi1, roi2$**:\n",
    "   - $roi1$: The valid region of interest in the rectified image for the first camera.\n",
    "   - $roi2$: The valid region of interest in the rectified image for the second camera.\n",
    "   - **Purpose**: These are the bounding rectangles that define the valid part of each rectified image where meaningful data exists post-rectification. It is useful for cropping out the borders of the rectified images that may not have valid information due to the lens distortion correction and image transformation.\n",
    "\n",
    "### Summary of Use\n",
    "\n",
    "- **Rectification Transforms ($R1, R2$)**: Aligns the stereo images for parallel epipolar lines.\n",
    "- **Projection Matrices ($P1, P2$)**: Adjusts the perspective projection to account for rectification.\n",
    "- **Disparity-to-Depth Mapping Matrix ($Q$)**: Converts disparity into 3D depth information.\n",
    "- **Regions of Interest ($roi1, roi2$)**: Defines usable regions in the rectified images.\n",
    "\n",
    "These components facilitate the preparation of stereo images for accurate stereo matching and depth estimation, ensuring that relevant geometrical constraints are satisfied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dive deeper in $Q$ Matrix\n",
    "\n",
    "The $Q$ matrix is derived from the relationship between the two rectified images and serves to transform the disparity value of a pixel into its real-world 3D coordinates. The concept hinges on understanding the geometry of stereo vision.\n",
    "\n",
    "### Geometry of Stereo Vision\n",
    "\n",
    "In stereo vision, each point in 3D space is projected onto two image planes. The disparity value for a point is the horizontal distance (in pixels) between its coordinates on the left and right images. Mathematically, if $(x_L, y_L)$ is the point in the left image and $(x_R, y_R)$ its corresponding point in the right image, then disparity $d$ is:\n",
    "\n",
    "$$ d = x_L - x_R $$\n",
    "\n",
    "### Reprojection with $Q$\n",
    "\n",
    "The $Q$ matrix transforms a point in the disparity image into a 3D point as follows:\n",
    "\n",
    "For a given pixel in the stereo images, the coordinates in the 3D world are calculated using:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "X \\\\\n",
    "Y \\\\\n",
    "Z \\\\\n",
    "W\n",
    "\\end{bmatrix}\n",
    "=\n",
    "Q\n",
    "\\begin{bmatrix}\n",
    "x' \\\\\n",
    "y' \\\\\n",
    "d \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $(x', y')$ are the pixel coordinates in the rectified image.\n",
    "- $d$ is the disparity.\n",
    "\n",
    "After transformation, the real-world coordinates $(X, Y, Z)$ are obtained by normalizing the resulting homogeneous coordinates, using $W$:\n",
    "\n",
    "$$ X = \\frac{X'}{W}, \\quad Y = \\frac{Y'}{W}, \\quad Z = \\frac{Z'}{W} $$\n",
    "\n",
    "### Components of $Q$\n",
    "\n",
    "The matrix $Q$ typically has the following structure:\n",
    "\n",
    "$$\n",
    "Q = \\begin{bmatrix}\n",
    "1 & 0 & 0 & -c_x \\\\\n",
    "0 & 1 & 0 & -c_y \\\\\n",
    "0 & 0 & 0 & f \\\\\n",
    "0 & 0 & -\\frac{1}{T} & \\left( \\frac{c_x - c_x'}{T} \\right)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $c_x, c_y$ are the coordinates of the principal point in the left camera.\n",
    "- $c_x'$ is the principal point in the right camera.\n",
    "- $f$ is the focal length (assuming the same for both cameras after rectification).\n",
    "- $T$ is the baseline distance between the two cameras.\n",
    "\n",
    "### Key Relationships\n",
    "\n",
    "- **Depth (Z-coordinate)**: Inversely proportional to disparity $d$. As the disparity decreases (objects are further and appear closer together in the images), the depth $Z$ increases.\n",
    "\n",
    "- **Horizontal and Vertical Coordinates (X and Y)**: Scaled to the principal point offsets, ensuring that measurements are translated into the correct perspective based on the camera geometry.\n",
    "\n",
    "This mathematical framework allows us to take a pixel location and its disparity from the stereo image pair and map them into real-world coordinates, forming the basis for 3D reconstruction and depth mapping in stereo vision systems. The precision and configuration of $Q$ are critical for accurate 3D modeling and understanding of the spatial environment captured by stereo cameras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map1_L:  (1296, 2304, 2)\n",
      "map2_L:  (1296, 2304)\n"
     ]
    }
   ],
   "source": [
    "# Compute the undistortion and rectification transformation map\n",
    "map1_L, map2_L = cv2.initUndistortRectifyMap(\n",
    "    camera_matrix_L, dist_coeffs_L, R1, P1, image_size, cv2.CV_16SC2\n",
    ")\n",
    "map1_R, map2_R = cv2.initUndistortRectifyMap(\n",
    "    camera_matrix_R, dist_coeffs_R, R2, P2, image_size, cv2.CV_16SC2\n",
    ")\n",
    "print(\"map1_L: \", map1_L.shape)\n",
    "print(\"map2_L: \", map2_L.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Transformation Maps:**\n",
    "   - $ \\text{map1\\_L}, \\text{map2\\_L} $ and $ \\text{map1\\_R}, \\text{map2\\_R} $ are the outputs for the left and right images, respectively. They are required by the `cv2.remap` function to actually perform the pixels' transformation.\n",
    "   - Each map consists of two components. \n",
    "     - `map1` contains the (x, y) coordinates in the source image from which pixels should be taken\n",
    "     - `map2` contains coefficients to compute the relative location with subpixel accuracy\n",
    "\n",
    "#### Purpose\n",
    "\n",
    "The primary purpose of these mappings is to correct distortions and apply the rectification transformations computed earlier. Applying these maps to the original images results in undistorted and rectified images where the epipolar geometry is simplified. This is crucial for accurate stereo correspondence searching, ultimately leading to more precise disparity maps and 3D reconstructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the stereo images (replace with actual paths)\n",
    "image_left = cv2.imread(\"left_image.jpg\")\n",
    "image_right = cv2.imread(\"right_image.jpg\")\n",
    "\n",
    "# Apply the rectification\n",
    "rectified_left = cv2.remap(image_left, map1_L, map2_L, cv2.INTER_LINEAR)\n",
    "rectified_right = cv2.remap(image_right, map1_R, map2_R, cv2.INTER_LINEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
